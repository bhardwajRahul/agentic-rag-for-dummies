{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e0fd1790",
      "metadata": {
        "id": "e0fd1790"
      },
      "source": [
        "# Agentic RAG for Dummies\n",
        "\n",
        "An advanced Retrieval-Augmented Generation (RAG) system that uses intelligent agents to retrieve and synthesize information from PDF documents."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7d06c15",
      "metadata": {
        "id": "c7d06c15"
      },
      "source": [
        "## 1. Dependencies Installation\n",
        "\n",
        "Install required packages for the RAG system.\n",
        "\n",
        "**Documentation:**\n",
        "- [LangGraph](https://langchain-ai.github.io/langgraph/) - Framework for building custom agents with low-level control\n",
        "- [LangChain](https://python.langchain.com/docs/get_started/introduction) - Framework to quick start agents with any model provider\n",
        "- [Qdrant](https://qdrant.tech/documentation/) - Vector database for similarity search\n",
        "- [Gradio](https://www.gradio.app/docs) - Web interface for ML models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "321dbfd3",
      "metadata": {
        "id": "321dbfd3"
      },
      "outputs": [],
      "source": [
        "#Upload the requirements.txt available in the project folder and run the cell\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd7c3e2d",
      "metadata": {
        "id": "fd7c3e2d"
      },
      "source": [
        "## 2. Environment Configuration\n",
        "\n",
        "Set up directory structure and environment variables for document processing.\n",
        "\n",
        "**What it does:**\n",
        "- Creates directories for storing PDFs, Markdown files, and parent chunks\n",
        "- Defines collection names for the vector database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9782958",
      "metadata": {
        "id": "c9782958"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Configuration\n",
        "DOCS_DIR = \"docs\"  # Directory containing your pdfs files\n",
        "MARKDOWN_DIR = \"markdown\" # Directory containing the pdfs converted to markdown\n",
        "PARENT_STORE_PATH = \"parent_store\"  # Directory for parent chunk JSON files\n",
        "CHILD_COLLECTION = \"document_child_chunks\"\n",
        "\n",
        "os.makedirs(DOCS_DIR, exist_ok=True)\n",
        "os.makedirs(MARKDOWN_DIR, exist_ok=True)\n",
        "os.makedirs(PARENT_STORE_PATH, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06eca5f1",
      "metadata": {
        "id": "06eca5f1"
      },
      "source": [
        "## 3. LLM Initialization\n",
        "\n",
        "Initialize the Large Language Model that will power the conversational agent.\n",
        "\n",
        "**What it does:**\n",
        "- Configures the LLM using Ollama (local inference)\n",
        "- Alternative example provided for Google Gemini\n",
        "\n",
        "**Documentation:**\n",
        "- [LangChain Ollama](https://python.langchain.com/docs/integrations/chat/ollama)\n",
        "- [LangChain Google GenAI](https://python.langchain.com/docs/integrations/chat/google_generative_ai)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e8989b7",
      "metadata": {
        "id": "7e8989b7"
      },
      "outputs": [],
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "# Initialize LLM\n",
        "llm = ChatOllama(model=\"qwen3:4b-instruct-2507-q4_K_M\", temperature=0)\n",
        "\n",
        "# Alternative (example): Google Gemini\n",
        "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = \"your-api-key-here\"\n",
        "# llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-exp\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5058f292",
      "metadata": {
        "id": "5058f292"
      },
      "source": [
        "## 4. Embeddings Setup\n",
        "\n",
        "Configure embedding models for semantic search using hybrid retrieval (dense + sparse).\n",
        "\n",
        "**What it does:**\n",
        "- **Dense embeddings**: Capture semantic meaning using neural networks\n",
        "- **Sparse embeddings**: Provide keyword-based matching (BM25 algorithm)\n",
        "\n",
        "**Documentation:**\n",
        "- [HuggingFace Embeddings](https://python.langchain.com/docs/integrations/text_embedding/huggingfacehub)\n",
        "- [FastEmbed Sparse](https://qdrant.github.io/fastembed/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f982c53",
      "metadata": {
        "id": "9f982c53"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_qdrant.fastembed_sparse import FastEmbedSparse\n",
        "\n",
        "# Dense embeddings for semantic understanding\n",
        "dense_embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
        ")\n",
        "\n",
        "# Sparse embeddings for keyword matching\n",
        "sparse_embeddings = FastEmbedSparse(\n",
        "    model_name=\"Qdrant/bm25\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc968a4c",
      "metadata": {
        "id": "bc968a4c"
      },
      "source": [
        "## 5. Vector Database Configuration\n",
        "\n",
        "Set up Qdrant vector database for storing and retrieving document embeddings.\n",
        "\n",
        "**What it does:**\n",
        "- Initializes local Qdrant client with file-based storage\n",
        "- Creates collections with both dense and sparse vector configurations\n",
        "- Enables hybrid search capabilities\n",
        "\n",
        "**Documentation:**\n",
        "- [LangChain Qdrant](https://python.langchain.com/docs/integrations/vectorstores/qdrant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11efc63b",
      "metadata": {
        "id": "11efc63b"
      },
      "outputs": [],
      "source": [
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models as qmodels\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "from langchain_qdrant.qdrant import RetrievalMode\n",
        "\n",
        "# Initialize Qdrant client (local file-based storage)\n",
        "client = QdrantClient(path=\"qdrant_db\")\n",
        "\n",
        "# Get embedding dimension\n",
        "embedding_dimension = len(dense_embeddings.embed_query(\"test\"))\n",
        "\n",
        "def ensure_collection(collection_name):\n",
        "    \"\"\"Create Qdrant collection if it doesn't exist\"\"\"\n",
        "    if not client.collection_exists(collection_name):\n",
        "        client.create_collection(\n",
        "            collection_name=collection_name,\n",
        "            vectors_config=qmodels.VectorParams(\n",
        "                size=embedding_dimension,\n",
        "                distance=qmodels.Distance.COSINE\n",
        "            ),\n",
        "            sparse_vectors_config={\n",
        "                \"sparse\": qmodels.SparseVectorParams()\n",
        "            },\n",
        "        )\n",
        "        print(f\"âœ“ Created collection: {collection_name}\")\n",
        "    else:\n",
        "        print(f\"âœ“ Collection already exists: {collection_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bec851a6",
      "metadata": {
        "id": "bec851a6"
      },
      "source": [
        "## 6. PDF to Markdown Conversion\n",
        "\n",
        "Convert PDF documents to Markdown format for better text extraction and processing.\n",
        "\n",
        "**What it does:**\n",
        "- Uses PyMuPDF to extract text from PDFs\n",
        "- Converts documents to clean Markdown format\n",
        "- Handles encoding issues and removes images\n",
        "- Skips already converted files unless overwrite is enabled\n",
        "\n",
        "**Note:** For more details on PDF conversion, refer to the `pdf_to_md.ipynb` notebook in the repository.\n",
        "\n",
        "**Documentation:**\n",
        "- [PyMuPDF](https://pymupdf.readthedocs.io/)\n",
        "- [PyMuPDF4LLM](https://github.com/pymupdf/PyMuPDF4LLM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06fd6518",
      "metadata": {
        "id": "06fd6518"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pymupdf.layout\n",
        "import pymupdf4llm\n",
        "from pathlib import Path\n",
        "import glob\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "def pdf_to_markdown(pdf_path, output_dir):\n",
        "    doc = pymupdf.open(pdf_path)\n",
        "    md = pymupdf4llm.to_markdown(doc, header=False, footer=False, page_separators=True, ignore_images=True, write_images=False, image_path=None)\n",
        "    md_cleaned = md.encode('utf-8', errors='surrogatepass').decode('utf-8', errors='ignore')\n",
        "    output_path = Path(output_dir) / Path(doc.name).stem\n",
        "    Path(output_path).with_suffix(\".md\").write_bytes(md_cleaned.encode('utf-8'))\n",
        "\n",
        "def pdfs_to_markdowns(path_pattern, overwrite: bool = False):\n",
        "    output_dir = Path(MARKDOWN_DIR)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for pdf_path in map(Path, glob.glob(path_pattern)):\n",
        "        md_path = (output_dir / pdf_path.stem).with_suffix(\".md\")\n",
        "        if overwrite or not md_path.exists():\n",
        "            pdf_to_markdown(pdf_path, output_dir)\n",
        "\n",
        "pdfs_to_markdowns(f\"{DOCS_DIR}/*.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6bfff03",
      "metadata": {
        "id": "e6bfff03"
      },
      "source": [
        "## 7. Document Indexing\n",
        "\n",
        "Implement parent-child chunking strategy for optimal retrieval performance.\n",
        "\n",
        "**What it does:**\n",
        "- Splits documents into hierarchical chunks (parent and child)\n",
        "- **Parent chunks**: Large context windows (2000-4000 chars) stored as JSON\n",
        "- **Child chunks**: Small searchable units (500 chars) stored in Qdrant\n",
        "- Merges small chunks and splits large ones for consistency\n",
        "- Creates bidirectional links between parent and child chunks\n",
        "\n",
        "**Chunking Strategy:**\n",
        "1. Split by Markdown headers (#, ##, ###)\n",
        "2. Merge chunks smaller than 2000 characters\n",
        "3. Split chunks larger than 4000 characters\n",
        "4. Create child chunks (500 chars) from each parent\n",
        "5. Store parent chunks in JSON files\n",
        "6. Index child chunks in vector database\n",
        "\n",
        "**Documentation:**\n",
        "- [LangChain Text Splitters](https://docs.langchain.com/oss/python/integrations/splitters)\n",
        "- [LangChain Split Markdown](https://docs.langchain.com/oss/python/integrations/splitters/markdown_header_metadata_splitter)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e103d352",
      "metadata": {
        "id": "e103d352"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "from pathlib import Path\n",
        "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
        "\n",
        "if client.collection_exists(CHILD_COLLECTION):\n",
        "    print(f\"Removing existing Qdrant collection: {CHILD_COLLECTION}\")\n",
        "    client.delete_collection(CHILD_COLLECTION)\n",
        "    ensure_collection(CHILD_COLLECTION)\n",
        "else:\n",
        "    ensure_collection(CHILD_COLLECTION)\n",
        "\n",
        "child_vector_store = QdrantVectorStore(\n",
        "    client=client,\n",
        "    collection_name=CHILD_COLLECTION,\n",
        "    embedding=dense_embeddings,\n",
        "    sparse_embedding=sparse_embeddings,\n",
        "    retrieval_mode=RetrievalMode.HYBRID,\n",
        "    sparse_vector_name=\"sparse\"\n",
        ")\n",
        "\n",
        "def index_documents():\n",
        "    headers_to_split_on = [(\"#\", \"H1\"), (\"##\", \"H2\"), (\"###\", \"H3\")]\n",
        "    parent_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
        "    child_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "\n",
        "    min_parent_size = 2000\n",
        "    max_parent_size = 4000\n",
        "\n",
        "    all_parent_pairs, all_child_chunks = [], []\n",
        "    md_files = sorted(glob.glob(os.path.join(MARKDOWN_DIR, \"*.md\")))\n",
        "\n",
        "    if not md_files:\n",
        "        print(f\"âš ï¸  No .md files found in {MARKDOWN_DIR}/\")\n",
        "        return\n",
        "\n",
        "    for doc_path_str in md_files:\n",
        "        doc_path = Path(doc_path_str)\n",
        "        print(f\"ðŸ“„ Processing: {doc_path.name}\")\n",
        "\n",
        "        try:\n",
        "            with open(doc_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                md_text = f.read()\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error reading {doc_path.name}: {e}\")\n",
        "            continue\n",
        "\n",
        "        parent_chunks = parent_splitter.split_text(md_text)\n",
        "        merged_parents = merge_small_parents(parent_chunks, min_parent_size)\n",
        "        split_parents = split_large_parents(merged_parents, max_parent_size, child_splitter)\n",
        "        cleaned_parents = clean_small_chunks(split_parents, min_parent_size)\n",
        "\n",
        "        for i, p_chunk in enumerate(cleaned_parents):\n",
        "            parent_id = f\"{doc_path.stem}_parent_{i}\"\n",
        "            p_chunk.metadata.update({\"source\": doc_path.stem + \".pdf\", \"parent_id\": parent_id})\n",
        "            all_parent_pairs.append((parent_id, p_chunk))\n",
        "            children = child_splitter.split_documents([p_chunk])\n",
        "            all_child_chunks.extend(children)\n",
        "\n",
        "    if not all_child_chunks:\n",
        "        print(\"âš ï¸ No child chunks to index\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nðŸ” Indexing {len(all_child_chunks)} child chunks into Qdrant...\")\n",
        "    try:\n",
        "        child_vector_store.add_documents(all_child_chunks)\n",
        "        print(\"âœ“ Child chunks indexed successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error indexing child chunks: {e}\")\n",
        "        return\n",
        "\n",
        "    print(f\"ðŸ’¾ Saving {len(all_parent_pairs)} parent chunks to JSON...\")\n",
        "    for item in os.listdir(PARENT_STORE_PATH):\n",
        "        os.remove(os.path.join(PARENT_STORE_PATH, item))\n",
        "\n",
        "    for parent_id, doc in all_parent_pairs:\n",
        "        doc_dict = {\"page_content\": doc.page_content, \"metadata\": doc.metadata}\n",
        "        filepath = os.path.join(PARENT_STORE_PATH, f\"{parent_id}.json\")\n",
        "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(doc_dict, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "def merge_small_parents(chunks, min_size):\n",
        "    if not chunks:\n",
        "        return []\n",
        "\n",
        "    merged, current = [], None\n",
        "\n",
        "    for chunk in chunks:\n",
        "        if current is None:\n",
        "            current = chunk\n",
        "        else:\n",
        "            current.page_content += \"\\n\\n\" + chunk.page_content\n",
        "            for k, v in chunk.metadata.items():\n",
        "                if k in current.metadata:\n",
        "                    current.metadata[k] = f\"{current.metadata[k]} -> {v}\"\n",
        "                else:\n",
        "                    current.metadata[k] = v\n",
        "\n",
        "        if len(current.page_content) >= min_size:\n",
        "            merged.append(current)\n",
        "            current = None\n",
        "\n",
        "    if current:\n",
        "        if merged:\n",
        "            merged[-1].page_content += \"\\n\\n\" + current.page_content\n",
        "            for k, v in current.metadata.items():\n",
        "                if k in merged[-1].metadata:\n",
        "                    merged[-1].metadata[k] = f\"{merged[-1].metadata[k]} -> {v}\"\n",
        "                else:\n",
        "                    merged[-1].metadata[k] = v\n",
        "        else:\n",
        "            merged.append(current)\n",
        "\n",
        "    return merged\n",
        "\n",
        "def split_large_parents(chunks, max_size, splitter):\n",
        "    split_chunks = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        if len(chunk.page_content) <= max_size:\n",
        "            split_chunks.append(chunk)\n",
        "        else:\n",
        "            large_splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=max_size,\n",
        "                chunk_overlap=splitter._chunk_overlap\n",
        "            )\n",
        "            sub_chunks = large_splitter.split_documents([chunk])\n",
        "            split_chunks.extend(sub_chunks)\n",
        "\n",
        "    return split_chunks\n",
        "\n",
        "def clean_small_chunks(chunks, min_size):\n",
        "    cleaned = []\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        if len(chunk.page_content) < min_size:\n",
        "            if cleaned:\n",
        "                cleaned[-1].page_content += \"\\n\\n\" + chunk.page_content\n",
        "                for k, v in chunk.metadata.items():\n",
        "                    if k in cleaned[-1].metadata:\n",
        "                        cleaned[-1].metadata[k] = f\"{cleaned[-1].metadata[k]} -> {v}\"\n",
        "                    else:\n",
        "                        cleaned[-1].metadata[k] = v\n",
        "            elif i < len(chunks) - 1:\n",
        "                chunks[i + 1].page_content = chunk.page_content + \"\\n\\n\" + chunks[i + 1].page_content\n",
        "                for k, v in chunk.metadata.items():\n",
        "                    if k in chunks[i + 1].metadata:\n",
        "                        chunks[i + 1].metadata[k] = f\"{v} -> {chunks[i + 1].metadata[k]}\"\n",
        "                    else:\n",
        "                        chunks[i + 1].metadata[k] = v\n",
        "            else:\n",
        "                cleaned.append(chunk)\n",
        "        else:\n",
        "            cleaned.append(chunk)\n",
        "\n",
        "    return cleaned\n",
        "\n",
        "index_documents()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02bbcc5b",
      "metadata": {
        "id": "02bbcc5b"
      },
      "source": [
        "## 8. Tools Definition\n",
        "\n",
        "Define retrieval tools that agents can use to search and retrieve document chunks.\n",
        "\n",
        "**What it does:**\n",
        "- **search_child_chunks**: Searches vector database for relevant small chunks\n",
        "- **retrieve_parent_chunks**: Retrieves full context from parent chunk JSON files\n",
        "- Binds tools to LLM for agentic function calling\n",
        "\n",
        "**Two-stage retrieval:**\n",
        "1. Agent searches child chunks (fast, semantic search)\n",
        "2. Agent retrieves parent chunks for full context (when needed)\n",
        "\n",
        "**Documentation:**\n",
        "- [LangChain Tools](https://docs.langchain.com/oss/python/langchain/tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8eac029",
      "metadata": {
        "id": "b8eac029"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def search_child_chunks(query: str, limit: int) -> str:\n",
        "    \"\"\"Search for the top K most relevant child chunks.\n",
        "\n",
        "    Args:\n",
        "        query: Search query string\n",
        "        limit: Maximum number of results to return\n",
        "    \"\"\"\n",
        "    try:\n",
        "        results = child_vector_store.similarity_search(query, k=limit, score_threshold=0.7)\n",
        "        if not results:\n",
        "            return \"NO_RELEVANT_CHUNKS\"\n",
        "\n",
        "        return \"\\n\\n\".join([\n",
        "            f\"Parent ID: {doc.metadata.get('parent_id', '')}\\n\"\n",
        "            f\"File Name: {doc.metadata.get('source', '')}\\n\"\n",
        "            f\"Content: {doc.page_content.strip()}\"\n",
        "            for doc in results\n",
        "        ])\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"RETRIEVAL_ERROR: {str(e)}\"\n",
        "\n",
        "@tool\n",
        "def retrieve_parent_chunks(parent_id: str) -> str:\n",
        "    \"\"\"Retrieve full parent chunks by their IDs.\n",
        "\n",
        "    Args:\n",
        "        parent_id: Parent chunk ID to retrieve\n",
        "    \"\"\"\n",
        "    file_name = parent_id if parent_id.lower().endswith(\".json\") else f\"{parent_id}.json\"\n",
        "    path = os.path.join(PARENT_STORE_PATH, file_name)\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "        return \"NO_PARENT_DOCUMENT\"\n",
        "\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    return (\n",
        "        f\"Parent ID: {parent_id}\\n\"\n",
        "        f\"File Name: {data.get('metadata', {}).get('source', 'unknown')}\\n\"\n",
        "        f\"Content: {data.get('page_content', '').strip()}\"\n",
        "    )\n",
        "\n",
        "# Bind tools to LLM\n",
        "llm_with_tools = llm.bind_tools([search_child_chunks, retrieve_parent_chunks])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "093d4ee3",
      "metadata": {
        "id": "093d4ee3"
      },
      "source": [
        "## 9. System Prompts\n",
        "\n",
        "Define system prompts that guide agent behavior throughout the RAG pipeline.\n",
        "\n",
        "**What it does:**\n",
        "- **Conversation Summary**: Extracts key topics and referenced file names from chat history\n",
        "- **Query Rewrite**: Rewrites unclear queries, splits multi-part questions, and incorporates follow-up context only when needed\n",
        "- **Orchestrator**: Forces retrieval before answering, uses compressed memory to avoid redundant searches and parent chunk re-fetches\n",
        "- **Fallback Response**: Synthesizes a best-effort answer from already-retrieved context when the operation limit is reached\n",
        "- **Context Compression**: Compresses prior research into a structured, query-focused summary organized by source file, including a gaps section for missing information\n",
        "- **Aggregation**: Merges multiple sub-answers into a single cohesive final response\n",
        "\n",
        "**Key behaviors:**\n",
        "- Query rewriting uses conversation context minimally; domain-specific terms are preserved as-is\n",
        "- Agent checks compressed memory before searching to avoid re-querying covered aspects or re-fetching known parent IDs\n",
        "- Retry with broader/rephrased query if no relevant documents are found\n",
        "- Fallback Response activates at the operation limit, using only previously retrieved content\n",
        "- Source attribution at the end of every response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b12809c6",
      "metadata": {
        "id": "b12809c6"
      },
      "outputs": [],
      "source": [
        "def get_conversation_summary_prompt() -> str:\n",
        "    return \"\"\"You are an expert conversation summarizer.\n",
        "\n",
        "Your task is to create a brief 1-2 sentence summary of the conversation (max 30-50 words).\n",
        "\n",
        "Include:\n",
        "- Main topics discussed\n",
        "- Important facts or entities mentioned\n",
        "- Any unresolved questions if applicable\n",
        "- Sources file name (e.g., file1.pdf) or documents referenced\n",
        "\n",
        "Exclude:\n",
        "- Greetings, misunderstandings, off-topic content.\n",
        "\n",
        "Output:\n",
        "- Return ONLY the summary.\n",
        "- Do NOT include any explanations or justifications.\n",
        "- If no meaningful topics exist, return an empty string.\n",
        "\"\"\"\n",
        "\n",
        "def get_rewrite_query_prompt() -> str:\n",
        "    return \"\"\"You are an expert query analyst and rewriter.\n",
        "\n",
        "Your task is to rewrite the current user query for optimal document retrieval, incorporating conversation context only when necessary.\n",
        "\n",
        "Rules:\n",
        "1. Self-contained queries:\n",
        "   - Always rewrite the query to be clear and self-contained\n",
        "   - If the query is a follow-up (e.g., \"what about X?\", \"and for Y?\"), integrate minimal necessary context from the summary\n",
        "   - Do not add information not present in the query or conversation summary\n",
        "\n",
        "2. Domain-specific terms:\n",
        "   - Product names, brands, proper nouns, or technical terms are treated as domain-specific\n",
        "   - For domain-specific queries, use conversation context minimally or not at all\n",
        "   - Use the summary only to disambiguate vague queries\n",
        "\n",
        "3. Grammar and clarity:\n",
        "   - Fix grammar, spelling errors, and unclear abbreviations\n",
        "   - Remove filler words and conversational phrases\n",
        "   - Preserve concrete keywords and named entities\n",
        "\n",
        "4. Multiple information needs:\n",
        "   - If the query contains multiple distinct, unrelated questions, split into separate queries (maximum 3)\n",
        "   - Each sub-query must remain semantically equivalent to its part of the original\n",
        "   - Do not expand, enrich, or reinterpret the meaning\n",
        "\n",
        "5. Failure handling:\n",
        "   - If the query intent is unclear or unintelligible, mark as \"unclear\"\n",
        "\n",
        "Input:\n",
        "- conversation_summary: A concise summary of prior conversation\n",
        "- current_query: The user's current query\n",
        "\n",
        "Output:\n",
        "- One or more rewritten, self-contained queries suitable for document retrieval\n",
        "\"\"\"\n",
        "\n",
        "def get_orchestrator_prompt() -> str:\n",
        "    return \"\"\"You are an expert retrieval-augmented assistant.\n",
        "\n",
        "Your task is to act as a researcher: search documents first, analyze the data, and then provide a comprehensive answer using ONLY the retrieved information.\n",
        "\n",
        "Rules:\n",
        "1. You MUST call 'search_child_chunks' before answering, unless the [COMPRESSED CONTEXT FROM PRIOR RESEARCH] already contains sufficient information.\n",
        "2. Ground every claim in the retrieved documents. If context is insufficient, state what is missing rather than filling gaps with assumptions.\n",
        "3. If no relevant documents are found, broaden or rephrase the query and search again. Repeat until satisfied or the operation limit is reached.\n",
        "\n",
        "Compressed Memory:\n",
        "When [COMPRESSED CONTEXT FROM PRIOR RESEARCH] is present â€”\n",
        "- Queries already listed: do not repeat them.\n",
        "- Parent IDs already listed: do not call `retrieve_parent_chunks` on them again.\n",
        "- Use it to identify what is still missing before searching further.\n",
        "\n",
        "Workflow:\n",
        "1. Check the compressed context. Identify what has already been retrieved and what is still missing.\n",
        "2. Search for 5-7 relevant excerpts using 'search_child_chunks' ONLY for uncovered aspects.\n",
        "3. If NONE are relevant, apply rule 3 immediately.\n",
        "4. For each relevant but fragmented excerpt, call 'retrieve_parent_chunks' ONE BY ONE â€” only for IDs not in the compressed context. Never retrieve the same ID twice.\n",
        "5. Once context is complete, provide a detailed answer omitting no relevant facts.\n",
        "6. Conclude with \"---\\n**Sources:**\\n\" followed by the unique file names.\n",
        "\"\"\"\n",
        "\n",
        "def get_fallback_response_prompt() -> str:\n",
        "    return \"\"\"You are an expert synthesis assistant. The system has reached its maximum research limit.\n",
        "\n",
        "Your task is to provide the most complete answer possible using ONLY the information provided below.\n",
        "\n",
        "Input structure:\n",
        "- \"Compressed Research Context\": summarized findings from prior search iterations â€” treat as reliable.\n",
        "- \"Retrieved Data\": raw tool outputs from the current iteration â€” prefer over compressed context if conflicts arise.\n",
        "Either source alone is sufficient if the other is absent.\n",
        "\n",
        "Rules:\n",
        "1. Source Integrity: Use only facts explicitly present in the provided context. Do not infer, assume, or add any information not directly supported by the data.\n",
        "2. Handling Missing Data: Cross-reference the USER QUERY against the available context.\n",
        "   Flag ONLY aspects of the user's question that cannot be answered from the provided data.\n",
        "   Do not treat gaps mentioned in the Compressed Research Context as unanswered\n",
        "   unless they are directly relevant to what the user asked.\n",
        "3. Tone: Professional, factual, and direct.\n",
        "4. Output only the final answer. Do not expose your reasoning, internal steps, or any meta-commentary about the retrieval process.\n",
        "5. Do NOT add closing remarks, final notes, disclaimers, summaries, or repeated statements after the Sources section.\n",
        "   The Sources section is always the last element of your response. Stop immediately after it.\n",
        "\n",
        "Formatting:\n",
        "- Use Markdown (headings, bold, lists) for readability.\n",
        "- Write in flowing paragraphs where possible.\n",
        "- Conclude with a Sources section as described below.\n",
        "\n",
        "Sources section rules:\n",
        "- Include a \"---\\\\n**Sources:**\\\\n\" section at the end, followed by a bulleted list of file names.\n",
        "- List ONLY entries that have a real file extension (e.g. \".pdf\", \".docx\", \".txt\").\n",
        "- Any entry without a file extension is an internal chunk identifier â€” discard it entirely, never include it.\n",
        "- Deduplicate: if the same file appears multiple times, list it only once.\n",
        "- If no valid file names are present, omit the Sources section entirely.\n",
        "- THE SOURCES SECTION IS THE LAST THING YOU WRITE. Do not add anything after it.\n",
        "\"\"\"\n",
        "\n",
        "def get_context_compression_prompt() -> str:\n",
        "    return \"\"\"You are an expert research context compressor.\n",
        "\n",
        "Your task is to compress retrieved conversation content into a concise, query-focused, and structured summary that can be directly used by a retrieval-augmented agent for answer generation.\n",
        "\n",
        "Rules:\n",
        "1. Keep ONLY information relevant to answering the user's question.\n",
        "2. Preserve exact figures, names, versions, technical terms, and configuration details.\n",
        "3. Remove duplicated, irrelevant, or administrative details.\n",
        "4. Do NOT include search queries, parent IDs, chunk IDs, or internal identifiers.\n",
        "5. Organize all findings by source file. Each file section MUST start with: ### filename.pdf\n",
        "6. Highlight missing or unresolved information in a dedicated \"Gaps\" section.\n",
        "7. Limit the summary to roughly 400-600 words. If content exceeds this, prioritize critical facts and structured data.\n",
        "8. Do not explain your reasoning; output only structured content in Markdown.\n",
        "\n",
        "Required Structure:\n",
        "\n",
        "# Research Context Summary\n",
        "\n",
        "## Focus\n",
        "[Brief technical restatement of the question]\n",
        "\n",
        "## Structured Findings\n",
        "\n",
        "### filename.pdf\n",
        "- Directly relevant facts\n",
        "- Supporting context (if needed)\n",
        "\n",
        "## Gaps\n",
        "- Missing or incomplete aspects\n",
        "\n",
        "The summary should be concise, structured, and directly usable by an agent to generate answers or plan further retrieval.\n",
        "\"\"\"\n",
        "\n",
        "def get_aggregation_prompt() -> str:\n",
        "    return \"\"\"You are an expert aggregation assistant.\n",
        "\n",
        "Your task is to combine multiple retrieved answers into a single, comprehensive and natural response that flows well.\n",
        "\n",
        "Rules:\n",
        "1. Write in a conversational, natural tone - as if explaining to a colleague.\n",
        "2. Use ONLY information from the retrieved answers.\n",
        "3. Do NOT infer, expand, or interpret acronyms or technical terms unless explicitly defined in the sources.\n",
        "4. Weave together the information smoothly, preserving important details, numbers, and examples.\n",
        "5. Be comprehensive - include all relevant information from the sources, not just a summary.\n",
        "6. If sources disagree, acknowledge both perspectives naturally (e.g., \"While some sources suggest X, others indicate Y...\").\n",
        "7. Start directly with the answer - no preambles like \"Based on the sources...\".\n",
        "\n",
        "Formatting:\n",
        "- Use Markdown for clarity (headings, lists, bold) but don't overdo it.\n",
        "- Write in flowing paragraphs where possible rather than excessive bullet points.\n",
        "- Conclude with a Sources section as described below.\n",
        "\n",
        "Sources section rules:\n",
        "- Each retrieved answer may contain a \"Sources\" section â€” extract the file names listed there.\n",
        "- List ONLY entries that have a real file extension (e.g. \".pdf\", \".docx\", \".txt\").\n",
        "- Any entry without a file extension is an internal chunk identifier â€” discard it entirely, never include it.\n",
        "- Deduplicate: if the same file appears across multiple answers, list it only once.\n",
        "- Format as \"---\\\\n**Sources:**\\\\n\" followed by a bulleted list of the cleaned file names.\n",
        "- File names must appear ONLY in this final Sources section and nowhere else in the response.\n",
        "- If no valid file names are present, omit the Sources section entirely.\n",
        "\n",
        "If there's no useful information available, simply say: \"I couldn't find any information to answer your question in the available sources.\"\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13764114",
      "metadata": {
        "id": "13764114"
      },
      "source": [
        "## 10. State Definitions\n",
        "\n",
        "Define state schemas for managing conversation flow and agent execution.\n",
        "\n",
        "**What it does:**\n",
        "- **State**: Tracks main conversation flow (query analysis, sub-questions, answers)\n",
        "- **AgentState**: Manages individual agent execution (current question, retrieved context, iteration tracking)\n",
        "- **QueryAnalysis**: Structured output for query rewriting and clarity checking\n",
        "\n",
        "**State management:**\n",
        "- `accumulate_or_reset`: Custom reducer for agent answers (allows reset)\n",
        "- `set_union`: Custom reducer for retrieval keys (merges sets via union)\n",
        "- Inherits from `MessagesState` for conversation history\n",
        "\n",
        "**Documentation:**\n",
        "- [LangGraph State](https://langchain-ai.github.io/langgraph/concepts/low_level/#state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4c066b1",
      "metadata": {
        "id": "c4c066b1"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import MessagesState\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Annotated, Set\n",
        "import operator\n",
        "\n",
        "def accumulate_or_reset(existing: List[dict], new: List[dict]) -> List[dict]:\n",
        "    if new and any(item.get('__reset__') for item in new):\n",
        "        return []\n",
        "    return existing + new\n",
        "\n",
        "def set_union(a: Set[str], b: Set[str]) -> Set[str]:\n",
        "    return a | b\n",
        "\n",
        "class State(MessagesState):\n",
        "    \"\"\"State for main agent graph\"\"\"\n",
        "    questionIsClear: bool = False\n",
        "    conversation_summary: str = \"\"\n",
        "    originalQuery: str = \"\"\n",
        "    rewrittenQuestions: List[str] = []\n",
        "    agent_answers: Annotated[List[dict], accumulate_or_reset] = []\n",
        "\n",
        "class AgentState(MessagesState):\n",
        "    \"\"\"State for individual agent subgraph\"\"\"\n",
        "    tool_call_count: Annotated[int, operator.add] = 0\n",
        "    iteration_count: Annotated[int, operator.add] = 0\n",
        "    question: str = \"\"\n",
        "    question_index: int = 0\n",
        "    context_summary: str = \"\"\n",
        "    retrieval_keys: Annotated[Set[str], set_union] = set()\n",
        "    final_answer: str = \"\"\n",
        "    agent_answers: List[dict] = []\n",
        "\n",
        "class QueryAnalysis(BaseModel):\n",
        "    is_clear: bool = Field(\n",
        "        description=\"Indicates if the user's question is clear and answerable.\"\n",
        "    )\n",
        "    questions: List[str] = Field(\n",
        "        description=\"List of rewritten, self-contained questions.\"\n",
        "    )\n",
        "    clarification_needed: str = Field(\n",
        "        description=\"Explanation if the question is unclear.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bb_8drzOiux",
      "metadata": {
        "id": "5bb_8drzOiux"
      },
      "source": [
        "## 11. Agent Limits and Token Utilities\n",
        "\n",
        "Define execution limits and token counting utilities for controlling agent loop behavior.\n",
        "\n",
        "**What it does:**\n",
        "- Sets hard limits on agent execution (tool calls, iterations)\n",
        "- Estimates token usage from message lists to drive context management decisions\n",
        "\n",
        "**Constants:**\n",
        "- `MAX_TOOL_CALLS`: Maximum number of tool calls per agent run\n",
        "- `MAX_ITERATIONS`: Maximum number of agent loop iterations\n",
        "- `BASE_TOKEN_THRESHOLD`: Initial token threshold for context summarization\n",
        "- `TOKEN_GROWTH_FACTOR`: Multiplier applied to threshold after each summarization\n",
        "\n",
        "**Functions:**\n",
        "- `estimate_context_tokens`: Counts tokens across a message list using `tiktoken`, falling back to `cl100k_base` encoding if the model-specific one is unavailable\n",
        "\n",
        "**Documentation:**\n",
        "- [tiktoken](https://github.com/openai/tiktoken)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VPKTdBgIOjw8",
      "metadata": {
        "id": "VPKTdBgIOjw8"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "MAX_TOOL_CALLS = 8\n",
        "MAX_ITERATIONS = 10\n",
        "BASE_TOKEN_THRESHOLD = 2000\n",
        "TOKEN_GROWTH_FACTOR = 0.9\n",
        "\n",
        "def estimate_context_tokens(messages: list) -> int:\n",
        "    \"\"\"Count tokens in message list\"\"\"\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "    except:\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "    total = 0\n",
        "    for msg in messages:\n",
        "        if hasattr(msg, 'content') and msg.content:\n",
        "            total += len(encoding.encode(str(msg.content)))\n",
        "    return total"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dda2930",
      "metadata": {
        "id": "7dda2930"
      },
      "source": [
        "## 12. Graph Nodes and Logic\n",
        "\n",
        "Implement node functions that define the behavior of the agentic workflow.\n",
        "\n",
        "**What it does:**\n",
        "\n",
        "### Core Nodes:\n",
        "1. **summarize_history**: Extracts and summarizes recent conversation context to support query rewriting; resets agent answers on each run\n",
        "2. **rewrite_query**: Rewrites and optionally splits the user query into sub-questions; checks clarity and removes stale messages if the query is clear\n",
        "3. **request_clarification**: Interrupt point for human-in-the-loop when the query is ambiguous\n",
        "4. **orchestrator**: Main agent loop â€” invokes the LLM with tools, injects compressed context when available, and forces an initial retrieval call\n",
        "5. **fallback_response**: Generates a best-effort answer directly from all retrieved tool data when the agent loop exhausts its budget\n",
        "6. **compress_context**: Summarizes and compresses the current message history and prior context summary into a single block, appending a log of already-executed retrievals to prevent redundant calls; removes compressed messages from state\n",
        "7. **collect_answer**: Extracts the final assistant answer from the agent message history and packages it with its index for aggregation\n",
        "8. **aggregate_answers**: Sorts and merges all sub-question answers, then synthesizes them into a single coherent response matching the original user query\n",
        "\n",
        "### Routing Logic:\n",
        "- **route_after_rewrite**: Routes to `request_clarification` if the query is unclear; otherwise uses `Send` to spawn parallel `agent` subgraphs â€” one per rewritten sub-question\n",
        "- **should_compress_context**: Uses `Command` to conditionally route to `compress_context` (when token usage exceeds a dynamic threshold) or back to `orchestrator`; also tracks all retrieval keys across iterations to avoid repeated tool calls\n",
        "\n",
        "**Documentation:**\n",
        "- [LangGraph Nodes](https://docs.langchain.com/oss/python/langgraph/graph-api#nodes)\n",
        "- [LangGraph Edges](https://docs.langchain.com/oss/python/langgraph/graph-api#edges)\n",
        "- [LangGraph Send API](https://docs.langchain.com/oss/python/langgraph/graph-api#send)\n",
        "- [LangGraph Command API](https://docs.langchain.com/oss/python/langgraph/graph-api#command)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "948ca8ee",
      "metadata": {
        "id": "948ca8ee"
      },
      "outputs": [],
      "source": [
        "from langgraph.types import Send, Command\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, RemoveMessage, ToolMessage\n",
        "from typing import Literal\n",
        "\n",
        "def summarize_history(state: State):\n",
        "    \"\"\"Analyzes chat history and summarizes key points for context.\"\"\"\n",
        "    if len(state[\"messages\"]) < 4:  # Need some history to summarize\n",
        "        return {\"conversation_summary\": \"\"}\n",
        "\n",
        "    # Extract relevant messages (excluding current query and system messages)\n",
        "    relevant_msgs = [\n",
        "        msg for msg in state[\"messages\"][:-1]  # Exclude current query\n",
        "        if isinstance(msg, (HumanMessage, AIMessage)) and not getattr(msg, \"tool_calls\", None)\n",
        "    ]\n",
        "\n",
        "    if not relevant_msgs:\n",
        "        return {\"conversation_summary\": \"\"}\n",
        "\n",
        "    conversation = \"Conversation history:\\n\"\n",
        "    for msg in relevant_msgs[-6:]:\n",
        "        role = \"User\" if isinstance(msg, HumanMessage) else \"Assistant\"\n",
        "        conversation += f\"{role}: {msg.content}\\n\"\n",
        "\n",
        "    summary_response = llm.with_config(temperature=0.2).invoke([SystemMessage(content=get_conversation_summary_prompt()), HumanMessage(content=conversation)])\n",
        "    return {\"conversation_summary\": summary_response.content, \"agent_answers\": [{\"__reset__\": True}]}\n",
        "\n",
        "def rewrite_query(state: State):\n",
        "    \"\"\"Analyzes user query and rewrites it for clarity, optionally using conversation context.\"\"\"\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    conversation_summary = state.get(\"conversation_summary\", \"\")\n",
        "\n",
        "    context_section = (f\"Conversation Context:\\n{conversation_summary}\\n\" if conversation_summary.strip() else \"\") + f\"User Query:\\n{last_message.content}\\n\"\n",
        "\n",
        "    llm_with_structure = llm.with_config(temperature=0.1).with_structured_output(QueryAnalysis)\n",
        "    response = llm_with_structure.invoke([SystemMessage(content=get_rewrite_query_prompt()), HumanMessage(content=context_section)])\n",
        "\n",
        "    if response.questions and response.is_clear:\n",
        "        delete_all = [RemoveMessage(id=m.id) for m in state[\"messages\"] if not isinstance(m, SystemMessage)]\n",
        "        return {\"questionIsClear\": True, \"messages\": delete_all, \"originalQuery\": last_message.content, \"rewrittenQuestions\": response.questions}\n",
        "\n",
        "    clarification = response.clarification_needed if response.clarification_needed and len(response.clarification_needed.strip()) > 10 else \"I need more information to understand your question.\"\n",
        "    return {\"questionIsClear\": False, \"messages\": [AIMessage(content=clarification)]}\n",
        "\n",
        "def request_clarification(state: State):\n",
        "    \"\"\"Placeholder node for human-in-the-loop interruption\"\"\"\n",
        "    return {}\n",
        "\n",
        "def route_after_rewrite(state: State) -> Literal[\"request_clarification\", \"agent\"]:\n",
        "    \"\"\"Route to agent if question is clear, otherwise wait for human input\"\"\"\n",
        "    if not state.get(\"questionIsClear\", False):\n",
        "        return \"request_clarification\"\n",
        "    else:\n",
        "        return [\n",
        "                Send(\"agent\", {\"question\": query, \"question_index\": idx, \"messages\": []})\n",
        "                for idx, query in enumerate(state[\"rewrittenQuestions\"])\n",
        "            ]\n",
        "\n",
        "# --- Agent Nodes ---\n",
        "def orchestrator(state: AgentState):\n",
        "    \"\"\"Main agent logic: decides when to search, retrieve, and answer based on context and tool calls.\"\"\"\n",
        "    context_summary = state.get(\"context_summary\", \"\").strip()\n",
        "    sys_msg = SystemMessage(content=get_orchestrator_prompt())\n",
        "    summary_injection = (\n",
        "        [HumanMessage(content=f\"[COMPRESSED CONTEXT FROM PRIOR RESEARCH]\\n\\n{context_summary}\")]\n",
        "        if context_summary else []\n",
        "    )\n",
        "    if not state.get(\"messages\"):\n",
        "        human_msg = HumanMessage(content=state[\"question\"])\n",
        "        force_search = HumanMessage(content=\"YOU MUST CALL 'search_child_chunks' AS THE FIRST STEP TO ANSWER THIS QUESTION.\")\n",
        "        response = llm_with_tools.invoke([sys_msg] + summary_injection + [human_msg, force_search])\n",
        "        return {\"messages\": [human_msg, response], \"tool_call_count\": len(response.tool_calls or []), \"iteration_count\": 1}\n",
        "\n",
        "    response = llm_with_tools.invoke([sys_msg] + summary_injection + state[\"messages\"])\n",
        "    tool_calls = response.tool_calls if hasattr(response, \"tool_calls\") else []\n",
        "    return {\"messages\": [response], \"tool_call_count\": len(tool_calls) if tool_calls else 0, \"iteration_count\": 1}\n",
        "\n",
        "def route_after_orchestrator_call(state: AgentState) -> Literal[\"tool\", \"fallback_response\", \"collect_answer\"]:\n",
        "    \"\"\"Determines next step after agent response: whether to call tools again, fallback, or collect answer.\"\"\"\n",
        "    iteration = state.get(\"iteration_count\", 0)\n",
        "    tool_count = state.get(\"tool_call_count\", 0)\n",
        "\n",
        "    if iteration >= MAX_ITERATIONS or tool_count > MAX_TOOL_CALLS:\n",
        "        return \"fallback_response\"\n",
        "\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    tool_calls = getattr(last_message, \"tool_calls\", None) or []\n",
        "\n",
        "    if not tool_calls:\n",
        "        return \"collect_answer\"\n",
        "\n",
        "    return \"tools\"\n",
        "\n",
        "def fallback_response(state: AgentState):\n",
        "    \"\"\"Generates a fallback answer using all retrieved information when iteration or tool call limits are reached.\"\"\"\n",
        "    seen = set()\n",
        "    unique_contents = []\n",
        "    for m in state[\"messages\"]:\n",
        "        if isinstance(m, ToolMessage) and m.content not in seen:\n",
        "            unique_contents.append(m.content)\n",
        "            seen.add(m.content)\n",
        "\n",
        "    context_summary = state.get(\"context_summary\", \"\").strip()\n",
        "\n",
        "    context_parts = []\n",
        "    if context_summary:\n",
        "        context_parts.append(f\"## Compressed Research Context (from prior iterations)\\n\\n{context_summary}\")\n",
        "    if unique_contents:\n",
        "        context_parts.append(\n",
        "            \"## Retrieved Data (current iteration)\\n\\n\" +\n",
        "            \"\\n\\n\".join(f\"--- DATA SOURCE {i} ---\\n{content}\" for i, content in enumerate(unique_contents, 1))\n",
        "        )\n",
        "\n",
        "    context_text = \"\\n\\n\".join(context_parts) if context_parts else \"No data was retrieved from the documents.\"\n",
        "\n",
        "    prompt_content = (\n",
        "        f\"USER QUERY: {state.get('question')}\\n\\n\"\n",
        "        f\"{context_text}\\n\\n\"\n",
        "        f\"INSTRUCTION:\\nProvide the best possible answer using only the data above.\"\n",
        "    )\n",
        "    response = llm.invoke([SystemMessage(content=get_fallback_response_prompt()), HumanMessage(content=prompt_content)])\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "def should_compress_context(state: AgentState) -> Command[Literal[\"compress_context\", \"orchestrator\"]]:\n",
        "    \"\"\"Determines whether to compress context based on token count and tool calls.\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    new_ids: Set[str] = set()\n",
        "    for msg in reversed(messages):\n",
        "        if isinstance(msg, AIMessage) and getattr(msg, \"tool_calls\", None):\n",
        "            for tc in msg.tool_calls:\n",
        "                if tc[\"name\"] == \"retrieve_parent_chunks\":\n",
        "                    raw = tc[\"args\"].get(\"parent_id\") or tc[\"args\"].get(\"id\") or tc[\"args\"].get(\"ids\") or []\n",
        "                    if isinstance(raw, str):\n",
        "                        new_ids.add(f\"parent::{raw}\")\n",
        "                    else:\n",
        "                        new_ids.update(f\"parent::{r}\" for r in raw)\n",
        "\n",
        "                elif tc[\"name\"] == \"search_child_chunks\":\n",
        "                    query = tc[\"args\"].get(\"query\", \"\")\n",
        "                    if query:\n",
        "                        new_ids.add(f\"search::{query}\")\n",
        "            break\n",
        "\n",
        "    updated_ids = state.get(\"retrieval_keys\", set()) | new_ids\n",
        "\n",
        "    current_token_messages = estimate_context_tokens(messages)\n",
        "    current_token_summary = estimate_context_tokens([HumanMessage(content=state.get(\"context_summary\", \"\"))])\n",
        "    current_tokens = current_token_messages + current_token_summary\n",
        "\n",
        "    max_allowed = BASE_TOKEN_THRESHOLD + int(current_token_summary * TOKEN_GROWTH_FACTOR)\n",
        "\n",
        "    goto = \"compress_context\" if current_tokens > max_allowed else \"orchestrator\"\n",
        "    return Command(update={\"retrieval_keys\": updated_ids}, goto=goto)\n",
        "\n",
        "def compress_context(state: AgentState):\n",
        "    \"\"\"Compresses conversation and retrieval history into a concise summary for future iterations.\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    existing_summary = state.get(\"context_summary\", \"\").strip()\n",
        "\n",
        "    if not messages:\n",
        "        return {}\n",
        "\n",
        "    conversation_text = f\"USER QUESTION:\\n{state.get('question')}\\n\\nConversation to compress:\\n\\n\"\n",
        "    if existing_summary:\n",
        "        conversation_text += f\"[PRIOR COMPRESSED CONTEXT]\\n{existing_summary}\\n\\n\"\n",
        "\n",
        "    for msg in messages[1:]:\n",
        "        if isinstance(msg, AIMessage):\n",
        "            tool_calls_info = \"\"\n",
        "            if getattr(msg, \"tool_calls\", None):\n",
        "                calls = \", \".join(f\"{tc['name']}({tc['args']})\" for tc in msg.tool_calls)\n",
        "                tool_calls_info = f\" | Tool calls: {calls}\"\n",
        "            conversation_text += f\"[ASSISTANT{tool_calls_info}]\\n{msg.content or '(tool call only)'}\\n\\n\"\n",
        "        elif isinstance(msg, ToolMessage):\n",
        "            tool_name = getattr(msg, \"name\", \"tool\")\n",
        "            conversation_text += f\"[TOOL RESULT â€” {tool_name}]\\n{msg.content}\\n\\n\"\n",
        "\n",
        "    summary_response = llm.invoke([SystemMessage(content=get_context_compression_prompt()), HumanMessage(content=conversation_text)])\n",
        "    new_summary = summary_response.content\n",
        "\n",
        "    retrieved_ids: Set[str] = state.get(\"retrieval_keys\", set())\n",
        "    if retrieved_ids:\n",
        "        parent_ids = sorted(r for r in retrieved_ids if r.startswith(\"parent::\"))\n",
        "        search_queries = sorted(r.replace(\"search::\", \"\") for r in retrieved_ids if r.startswith(\"search::\"))\n",
        "\n",
        "        block = \"\\n\\n---\\n**Already executed (do NOT repeat):**\\n\"\n",
        "        if parent_ids:\n",
        "            block += \"Parent chunks retrieved:\\n\" + \"\\n\".join(f\"- {p.replace('parent::', '')}\" for p in parent_ids) + \"\\n\"\n",
        "        if search_queries:\n",
        "            block += \"Search queries already run:\\n\" + \"\\n\".join(f\"- {q}\" for q in search_queries) + \"\\n\"\n",
        "        new_summary += block\n",
        "\n",
        "    return {\"context_summary\": new_summary, \"messages\": [RemoveMessage(id=m.id) for m in messages[1:]]}\n",
        "\n",
        "def collect_answer(state: AgentState):\n",
        "    \"\"\"Collects the final answer from the agent's messages after retrieval is complete.\"\"\"\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    is_valid = isinstance(last_message, AIMessage) and last_message.content and not last_message.tool_calls\n",
        "    answer = last_message.content if is_valid else \"Unable to generate an answer.\"\n",
        "    return {\n",
        "        \"final_answer\": answer,\n",
        "        \"agent_answers\": [{\"index\": state[\"question_index\"], \"question\": state[\"question\"], \"answer\": answer}]\n",
        "    }\n",
        "# --- End of Agent Nodes---\n",
        "\n",
        "def aggregate_answers(state: State):\n",
        "    \"\"\"Aggregates multiple answers from different agent iterations into a single comprehensive response.\"\"\"\n",
        "    if not state.get(\"agent_answers\"):\n",
        "        return {\"messages\": [AIMessage(content=\"No answers were generated.\")]}\n",
        "\n",
        "    sorted_answers = sorted(state[\"agent_answers\"], key=lambda x: x[\"index\"])\n",
        "\n",
        "    formatted_answers = \"\"\n",
        "    for i, ans in enumerate(sorted_answers, start=1):\n",
        "        formatted_answers += (f\"\\nAnswer {i}:\\n\"f\"{ans['answer']}\\n\")\n",
        "\n",
        "    user_message = HumanMessage(content=f\"\"\"Original user question: {state[\"originalQuery\"]}\\nRetrieved answers:{formatted_answers}\"\"\")\n",
        "    synthesis_response = llm.invoke([SystemMessage(content=get_aggregation_prompt()), user_message])\n",
        "    return {\"messages\": [AIMessage(content=synthesis_response.content)]}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "031b09b6",
      "metadata": {
        "id": "031b09b6"
      },
      "source": [
        "## 13. LangGraph Construction\n",
        "\n",
        "Construct the agentic workflow using LangGraph's state machine.\n",
        "\n",
        "**What it does:**\n",
        "- Builds a hierarchical graph with a main flow and a compiled agent subgraph\n",
        "- **Agent subgraph**: Handles individual sub-question retrieval and reasoning in a tool loop, with context compression and fallback handling\n",
        "- **Main graph**: Orchestrates conversation flow, query analysis, parallel agent execution, and response aggregation\n",
        "\n",
        "**Agent Subgraph Flow:**\n",
        "1. START â†’ `orchestrator` (LLM + tools)\n",
        "2. Route: tool call â†’ `tools` â†’ `should_compress_context` â†’ compress or loop back to `orchestrator`\n",
        "3. Route: iteration limit hit â†’ `fallback_response` â†’ `collect_answer`\n",
        "4. Route: final answer ready â†’ `collect_answer` â†’ END\n",
        "\n",
        "**Main Graph Flow:**\n",
        "1. START â†’ `summarize_history`\n",
        "2. `rewrite_query` â€” rewrites and validates the query\n",
        "3. Route: unclear â†’ `request_clarification` (interrupt) â†’ back to `rewrite_query` | clear â†’ spawn parallel `agent` subgraphs via `Send`\n",
        "4. All agents complete â†’ `aggregate_answers`\n",
        "5. END\n",
        "\n",
        "**Human-in-the-loop:** Graph interrupts *before* `request_clarification` if the query is unclear, resuming at `rewrite_query` once the user provides input\n",
        "\n",
        "**Key design decisions:**\n",
        "- `should_compress_context` is a node (not just an edge) because it uses `Command` to conditionally update state and route in a single step\n",
        "- `agent_subgraph` is compiled independently and embedded as a node in the main graph, enabling clean state isolation per sub-question\n",
        "- `InMemorySaver` checkpointer enables short-term memory across conversation turns\n",
        "\n",
        "**Documentation:**\n",
        "- [LangGraph StateGraph](https://docs.langchain.com/oss/python/langgraph/graph-api#stategraph)\n",
        "- [LangGraph Short-term Memory](https://docs.langchain.com/oss/python/langgraph/add-memory#add-short-term-memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8c111af",
      "metadata": {
        "id": "d8c111af"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import START, END, StateGraph\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# Initialize checkpointer\n",
        "checkpointer = InMemorySaver()\n",
        "\n",
        "# Build agent subgraph (handles individual questions)\n",
        "agent_builder = StateGraph(AgentState)\n",
        "agent_builder.add_node(orchestrator)\n",
        "agent_builder.add_node(\"tools\", ToolNode([search_child_chunks, retrieve_parent_chunks]))\n",
        "agent_builder.add_node(compress_context)\n",
        "agent_builder.add_node(fallback_response)\n",
        "agent_builder.add_node(should_compress_context)\n",
        "agent_builder.add_node(collect_answer)\n",
        "\n",
        "agent_builder.add_edge(START, \"orchestrator\")\n",
        "agent_builder.add_conditional_edges(\"orchestrator\", route_after_orchestrator_call, {\"tools\": \"tools\", \"fallback_response\": \"fallback_response\", \"collect_answer\": \"collect_answer\"})\n",
        "agent_builder.add_edge(\"tools\", \"should_compress_context\")\n",
        "agent_builder.add_edge(\"compress_context\", \"orchestrator\")\n",
        "agent_builder.add_edge(\"fallback_response\", \"collect_answer\")\n",
        "agent_builder.add_edge(\"collect_answer\", END)\n",
        "agent_subgraph = agent_builder.compile()\n",
        "\n",
        "# Build main graph (orchestrates workflow)\n",
        "graph_builder = StateGraph(State)\n",
        "graph_builder.add_node(summarize_history)\n",
        "graph_builder.add_node(rewrite_query)\n",
        "graph_builder.add_node(request_clarification)\n",
        "graph_builder.add_node(\"agent\", agent_subgraph)\n",
        "graph_builder.add_node(aggregate_answers)\n",
        "\n",
        "graph_builder.add_edge(START, \"summarize_history\")\n",
        "graph_builder.add_edge(\"summarize_history\", \"rewrite_query\")\n",
        "graph_builder.add_conditional_edges(\"rewrite_query\", route_after_rewrite)\n",
        "graph_builder.add_edge(\"request_clarification\", \"rewrite_query\")\n",
        "graph_builder.add_edge([\"agent\"], \"aggregate_answers\")\n",
        "graph_builder.add_edge(\"aggregate_answers\", END)\n",
        "\n",
        "# Compile graph\n",
        "agent_graph = graph_builder.compile(checkpointer=checkpointer, interrupt_before=[\"request_clarification\"])\n",
        "\n",
        "display(Image(agent_graph.get_graph(xray=True).draw_mermaid_png()))\n",
        "print(\"âœ“ Agent graph compiled successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b95d153",
      "metadata": {
        "id": "4b95d153"
      },
      "source": [
        "## 14. Gradio Interface\n",
        "\n",
        "Create an interactive web interface for chatting with the RAG agent.\n",
        "\n",
        "**What it does:**\n",
        "- Provides a chat interface using Gradio\n",
        "- Manages conversation threads with unique IDs\n",
        "- Handles human-in-the-loop interactions seamlessly\n",
        "- Automatically resumes interrupted workflows when user provides clarification\n",
        "\n",
        "**Features:**\n",
        "- Thread-based conversation persistence\n",
        "- Clear session functionality\n",
        "- Automatic state management via checkpointer\n",
        "- Citrus theme for modern UI\n",
        "\n",
        "**Note:** For a complete end-to-end pipeline with document ingestion UI, refer to the full application in the project repository.\n",
        "\n",
        "**Documentation:**\n",
        "- [Gradio ChatInterface](https://www.gradio.app/docs/gradio/chatinterface)\n",
        "- [Gradio Blocks](https://www.gradio.app/docs/blocks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5aadf1ed",
      "metadata": {
        "id": "5aadf1ed"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import uuid\n",
        "\n",
        "def create_thread_id():\n",
        "    \"\"\"Generate a unique thread ID for each conversation\"\"\"\n",
        "    return {\"configurable\": {\"thread_id\": str(uuid.uuid4())}, \"recursion_limit\": 50}\n",
        "\n",
        "def clear_session():\n",
        "    \"\"\"Clear thread for new conversation and clean up checkpointer state\"\"\"\n",
        "    global config\n",
        "    agent_graph.checkpointer.delete_thread(config[\"configurable\"][\"thread_id\"])\n",
        "    config = create_thread_id()\n",
        "\n",
        "def chat_with_agent(message, history):\n",
        "    \"\"\"\n",
        "    Handle chat with human-in-the-loop support.\n",
        "    Returns: response text\n",
        "    \"\"\"\n",
        "    current_state = agent_graph.get_state(config)\n",
        "    if current_state.next:\n",
        "        agent_graph.update_state(config,{\"messages\": [HumanMessage(content=message.strip())]})\n",
        "        result = agent_graph.invoke(None, config)\n",
        "    else:\n",
        "        result = agent_graph.invoke({\"messages\": [HumanMessage(content=message.strip())]}, config)\n",
        "    return result['messages'][-1].content\n",
        "\n",
        "# Initialize thread configuration\n",
        "config = create_thread_id()\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot(height=600, placeholder=\"<strong>Ask me anything!</strong><br><em>I'll search, reason, and act to give you the best answer :)</em>\")\n",
        "    chatbot.clear(clear_session)\n",
        "    gr.ChatInterface(fn=chat_with_agent, chatbot=chatbot)\n",
        "\n",
        "print(\"\\nLaunching application...\")\n",
        "demo.launch(theme=gr.themes.Citrus())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv (3.13.7)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
